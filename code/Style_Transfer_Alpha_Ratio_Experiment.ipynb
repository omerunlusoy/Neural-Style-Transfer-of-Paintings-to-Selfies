{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhTTNJBG9lv",
        "outputId": "c7c9b4ad-c3df-43ac-a275-114e6434051f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models  # torchvision package contains many types of datasets (including MNIST dataset) and pre-trained models\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import timedelta, datetime\n",
        "from PIL import Image\n",
        "import cv2  # for video\n",
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kaggle API\n",
        "# !pip install -q kaggle\n",
        "# upload kaggle.json\n",
        "# files.upload()\n",
        "\n",
        "# create a kaggle folder\n",
        "# ! mkdir ~/.kaggle\n",
        "# copy the kaggle.json to folder created\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# permission for the json to act\n",
        "#Â ! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# to list all datasets in kaggle\n",
        "# ! kaggle datasets list\n",
        "# Van Gogh Paintings\n",
        "# ! unzip .zip"
      ],
      "metadata": {
        "id": "aQH_vBfQMFbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXSuOR-lQIJN"
      },
      "source": [
        "******************************************************************************\n",
        "\n",
        "Supporting_Functions\n",
        "\n",
        "******************************************************************************\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i12ur9K1P1Yt"
      },
      "outputs": [],
      "source": [
        "def load_image(path, max_size=600, shape=None):\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        size = max(image.size)\n",
        "\n",
        "        if size > max_size:\n",
        "            size = max_size\n",
        "\n",
        "        if shape is not None:\n",
        "            size = shape\n",
        "\n",
        "        # transform image to be compatible with the model\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "        image = transform(image).unsqueeze(0)  # to add extra dimensionality\n",
        "        return image\n",
        "\n",
        "def select_image_from_dataset(dir_artists, dir_van_gogh):\n",
        "      # choice = input(\"Van Gogh(V) or All Artists(A):\")\n",
        "      choice = 'A'\n",
        "      if choice == 'V':\n",
        "        for i, filename in enumerate(os.listdir(dir_van_gogh)):\n",
        "          print(filename)\n",
        "        artist = input(\"Select type:\")\n",
        "        source_folder = os.path.join(dir_van_gogh, artist)\n",
        "        image = random.choice(os.listdir(source_folder))        \n",
        "        while not (image.endswith(\".jpg\") or image.endswith(\".jpeg\")):\n",
        "          image = random.choice(os.listdir(source_folder))\n",
        "        print('selected image name:', image)\n",
        "        return os.path.join(source_folder, image)\n",
        "      else:\n",
        "        for i, filename in enumerate(os.listdir(dir_artists)):\n",
        "          print(filename)\n",
        "        # artist = input(\"Select artist:\")\n",
        "        artist = 'Afremov'\n",
        "        source_folder = os.path.join(dir_artists, artist)\n",
        "        image = random.choice(os.listdir(source_folder))        \n",
        "        while not (image.endswith(\".jpg\") or image.endswith(\".jpeg\")):\n",
        "          image = random.choice(os.listdir(source_folder))\n",
        "        print('selected image name:', image)\n",
        "        return os.path.join(source_folder, image)\n",
        "\n",
        "def plot_images_bunch(images, names, nrows=1):\n",
        "      \n",
        "      fig, axeslist = plt.subplots(ncols=int(math.ceil(len(images)/nrows)), nrows=nrows)\n",
        "      for i in range(len(images)):\n",
        "        axeslist.ravel()[i].imshow(SF.image_convert_to_numpy(images[i]))\n",
        "        axeslist.ravel()[i].set_title(names[i])\n",
        "        axeslist.ravel()[i].set_axis_off()\n",
        "      plt.tight_layout()\n",
        "      if len(images) > 2:\n",
        "        plt.savefig('all_images.jpg', dpi=500, bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "# this class includes some general supporting functions for CNN Transfer Learning\n",
        "class Supporting_Functions:\n",
        "    def __init__(self, log_filename, content_image_path, style_image_path, device, content_weight, style_weight, learning_rate, steps, storage_limit):\n",
        "        super().__init__()\n",
        "        self.log_filename = log_filename\n",
        "        self.content_image_path = content_image_path\n",
        "        self.style_image_path = style_image_path\n",
        "        self.device = device\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.learning_rate = learning_rate\n",
        "        self.steps = steps\n",
        "        self.storage_limit = storage_limit\n",
        "\n",
        "    def enter_log(self, text, header=False):\n",
        "        file = open(self.log_filename, \"a\")\n",
        "\n",
        "        if header:\n",
        "            log_str = str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + \"\\nContent Image: \" + self.content_image_path + \"\\nStyle Image: \" + self.style_image_path \\\n",
        "                      + \"\\ndevice: \" + str(self.device) + \", content_weight: \" + str(self.content_weight) + \", style_weight: \" + str(self.style_weight) + \", learning_rate: \" \\\n",
        "                      + str(self.learning_rate) + \", steps: \" + str(self.steps) + \", storage_limit: \" + str(self.storage_limit)\n",
        "            file.write(\"\\n\\n\" + log_str + \"\\n\\n\")\n",
        "\n",
        "        file.write(text + \"\\n\")\n",
        "        file.close()\n",
        "\n",
        "    # 1st dimension: color, 2nd dimension: width, 3rd dimension: height of image and pixels\n",
        "    def image_convert_to_numpy(self, tensor):\n",
        "        image = tensor.clone().detach().cpu().numpy()  # clones to tensor and transforms to numpy array. OR tensor.cpu().clone().detach().numpy()\n",
        "        image = image.squeeze()\n",
        "        image = image.transpose(1, 2, 0)\n",
        "        # print(image.shape)                                                                            # (28, 28, 1)\n",
        "        # denormalize image\n",
        "        image = image * np.array((0.5,)) + np.array((0.5,))\n",
        "        image = image.clip(0, 1)\n",
        "        return image\n",
        "\n",
        "    def create_video(self, images, name):\n",
        "      frame_per_sec = 30\n",
        "      frame_height, frame_width, _ = images[0].shape\n",
        "      video = cv2.VideoWriter(name + '.avi', cv2.VideoWriter_fourcc(*'DIVX'), frame_per_sec, (frame_width, frame_height))\n",
        "\n",
        "      for i in range(len(images)):\n",
        "        cur_image = images[i]\n",
        "        # make current image RGB\n",
        "        cur_image = cur_image * 255\n",
        "        cur_image = np.array(cur_image, dtype=np.uint8)\n",
        "        cur_image = cv2.cvtColor(cur_image, cv2.COLOR_BGR2RGB)\n",
        "        video.write(cur_image)\n",
        "\n",
        "      video.release()\n",
        "      # video is fully written and ready to be saved\n",
        "      # cv2.destroyAllWindows()\n",
        "      SF.enter_log('Video created.')\n",
        "\n",
        "    def plot_images(self, images, nrows=1):\n",
        "      names = ['Content Image', 'Style Image', 'Target Image']\n",
        "      fig, axeslist = plt.subplots(ncols=int(len(images)/nrows), nrows=nrows)\n",
        "      for i in range(len(images)):\n",
        "        axeslist.ravel()[i].imshow(SF.image_convert_to_numpy(images[i]))\n",
        "        axeslist.ravel()[i].set_title(names[i])\n",
        "        axeslist.ravel()[i].set_axis_off()\n",
        "      plt.tight_layout()\n",
        "      if len(images) > 2:\n",
        "        plt.savefig('three_images.jpg', dpi=500, bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "    def save_image(self, image, name):\n",
        "      # save_image(image, fp=name, normalize=True)\n",
        "      plt.imshow(SF.image_convert_to_numpy(image))\n",
        "      plt.axis('off')\n",
        "      plt.savefig(name, dpi=500, bbox_inches='tight')\n",
        "      # plt.show()\n",
        "\n",
        "    \n",
        "      \n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMG9OeoQoQ1"
      },
      "source": [
        "******************************************************************************\n",
        "\n",
        "Training Algorithm\n",
        "\n",
        "******************************************************************************\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moVvFHPqRIHr"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "# assign weight to each style layer for representation power (early layers have more style)\n",
        "style_weights = {'conv1_1': 1.0,\n",
        "                 'conv2_1': 0.78,\n",
        "                 'conv3_1': 0.25,\n",
        "                 'conv4_1': 0.22,\n",
        "                 'conv5_1': 0.18}\n",
        "\n",
        "# dictionary that holds the specific layer numbers where features will be extracted. You can play with them.\n",
        "# Conv1_1, Conv2_1, Conv3_1, Conv4_1, Conv4_2, Conv5_1\n",
        "layers = {'0': 'conv1_1',    # style extraction\n",
        "          '5': 'conv2_1',    # style extraction\n",
        "          '10': 'conv3_1',   # style extraction\n",
        "          '19': 'conv4_1',   # style extraction\n",
        "          '21': 'conv4_2',   # content extraction\n",
        "          '28': 'conv5_1'}   # style extraction\n",
        "\n",
        "content_layer = 'conv4_2'\n",
        "\n",
        "\n",
        "# AUXILARY PARAMETERS\n",
        "# step number\n",
        "storage_limit = 300\n",
        "\n",
        "# print variables\n",
        "print_per = 10\n",
        "show_per = 50\n",
        "\n",
        "\n",
        "# datasets\n",
        "dataset_directory_artists = '/content/drive/MyDrive/CS-464 Project/CNN_files/Artists'\n",
        "dataset_directory_van_gogh = '/content/drive/MyDrive/CS-464 Project/CNN_files/Van_Gogh'\n",
        "\n",
        "content_image_path = '/content/drive/MyDrive/CS-464 Project/CNN_files/HayleyWilliams.jpg'\n",
        "# style_image_path = '/content/drive/MyDrive/CS-464 Project/CNN_files/AngelOfLove.jpeg'\n",
        "log_filename = \"/content/drive/MyDrive/CS-464 Project/CNN_files/log_CNN_Style_Transfer.txt\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # specifies run device for more optimum runtime\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "# returns pre-trained VGG19 model\n",
        "def get_model():\n",
        "    # VGG 19 pre-trained model\n",
        "    model = models.vgg19(pretrained=True).features\n",
        "    # SF.enter_log('VGG 19 pre-trained model is created.')\n",
        "\n",
        "    # freeze parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # send model to GPU\n",
        "    model.to(device=device)\n",
        "    return model\n",
        "\n",
        "\n",
        "# extracts the features from image using model\n",
        "def get_features(image, model):\n",
        "    # dict that will store the extracted features\n",
        "    features = {}\n",
        "    # iterate through all layers and store the on es in the layers dict\n",
        "    for name, layer in model._modules.items():\n",
        "        # run image through all layers\n",
        "        image = layer(image)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = image\n",
        "    return features\n",
        "\n",
        "\n",
        "# Gram Matrix = V(T)*V  T: Transpose\n",
        "def gram_matrix(tensor):\n",
        "    # takes 4D image tensor\n",
        "    # reshape the tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "    tensor = tensor.view(d, h*w)\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    return gram\n",
        "\n",
        "\n",
        "def train(model, content_image, style_image, steps, content_weight, style_weight, learning_rate):\n",
        "    # get content and style features\n",
        "    content_features = get_features(content_image, model)\n",
        "    style_features = get_features(style_image, model)\n",
        "\n",
        "    # style features need one more step to be more useful (Gram Matrix)\n",
        "    # applying Gram Matrix eliminates the remaining content information from style features\n",
        "    # Gram Matrix = V(T)*V  T: Transpose\n",
        "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    # define target image\n",
        "    target_image = content_image.clone().requires_grad_(True).to(device=device)\n",
        "\n",
        "    # training process images\n",
        "    height, width, channels = SF.image_convert_to_numpy(target_image).shape\n",
        "    images = np.empty(shape=(storage_limit, height, width, channels))\n",
        "\n",
        "    # Adam Optimizer\n",
        "    optimizer = torch.optim.Adam([target_image], lr=learning_rate)\n",
        "\n",
        "    # training process\n",
        "    SF.enter_log('Training begins...')\n",
        "    iter = 0\n",
        "    start_training_time = time.time()\n",
        "    for ii in range(1, steps + 1):\n",
        "        target_features = get_features(target_image, model)\n",
        "        # calculate the content loss between content and target images using Mean Squared Error\n",
        "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer]) ** 2)\n",
        "\n",
        "        # calculate style loss iterating through 5 style layers\n",
        "        style_loss = 0\n",
        "        for style_layer in style_weights:\n",
        "            # calculate target gram for cur layer\n",
        "            target_feature = target_features[style_layer]\n",
        "            target_gram = gram_matrix(target_feature)\n",
        "            # get corresponding style gram from the precalculated list\n",
        "            style_gram = style_grams[style_layer]\n",
        "            current_style_loss = style_weights[style_layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "            # normalize current_style_loss\n",
        "            _, d, h, w = target_feature.shape\n",
        "            style_loss += current_style_loss / (d * h * w)\n",
        "\n",
        "        # optimizer will be used to optimize the parameters of the target image according to content and style losses\n",
        "        # Style Aim: is to match the target gram matrix to the style gram matrix\n",
        "        # Content Aim: is to match the target features (filtered image) to the content features\n",
        "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "        optimizer.zero_grad()  # reset optimizer\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # data visualization throughout the training process\n",
        "        # print period\n",
        "        if ii % print_per == 0:\n",
        "            finish_training_time = time.time()\n",
        "            print_str = 'iteration: ' + str(ii) + ' loss: ' + str(total_loss.item() / (height * width)) + ' time passed: ' + str(timedelta(seconds=finish_training_time - start_training_time))\n",
        "            print(print_str)\n",
        "            SF.enter_log(print_str)\n",
        "\n",
        "        # show image period\n",
        "        if ii % show_per == 0:\n",
        "            plt.imshow(SF.image_convert_to_numpy(target_image))\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        # store mid images period\n",
        "        if ii % (steps / storage_limit) == 0:\n",
        "            images[iter] = SF.image_convert_to_numpy(target_image)\n",
        "            iter += 1\n",
        "\n",
        "    SF.enter_log('Training completed.')\n",
        "    return target_image, images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-qMLGRNQxKM"
      },
      "source": [
        "******************************************************************************\n",
        "\n",
        "Main Function\n",
        "\n",
        "******************************************************************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GASCBUY9RlY9",
        "outputId": "0ca6cd8a-4241-4907-a855-965cd56a0624"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "# style content ratio: alpha represents content image weight and beta represents style image weight\n",
        "content_weight = 1\n",
        "style_weight_ = 1e6      # (1e1, 1e6)\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 0.004\n",
        "\n",
        "# steps, iterations\n",
        "steps = 2100\n",
        "# additional_steps = 100\n",
        "\n",
        "style_weights_ = [1e6, 1e4, 1e2, 1e1]\n",
        "\n",
        "# MAIN\n",
        "\n",
        "# get content and style images\n",
        "content_image = load_image(content_image_path).to(device=device)\n",
        "style_image_path = select_image_from_dataset(dataset_directory_artists, dataset_directory_van_gogh)\n",
        "style_image = load_image(style_image_path, shape=content_image.shape[-2:]).to(device=device)\n",
        "\n",
        "# get VGG19 pre-trained model\n",
        "model = get_model()\n",
        "\n",
        "target_images = []\n",
        "names = ['Content Image', 'Style Image', 'Target with sw: 1e6', 'Target with sw: 1e4', 'Target with sw: 1e2', 'Target with sw: 1e1']\n",
        "\n",
        "for style_weight in style_weights_:\n",
        "  print('Training is started for', style_weight)\n",
        "  SF = Supporting_Functions(log_filename, content_image_path, '/from dataset', device, content_weight, style_weight, learning_rate, steps, storage_limit)\n",
        "  SF.enter_log('Content and Style images are retrieved.', header=True)\n",
        "\n",
        "  # plot content and style images\n",
        "  SF.plot_images([content_image, style_image])\n",
        "\n",
        "  # train target image\n",
        "  target_image, images = train(model, content_image, style_image, steps, content_weight, style_weight, learning_rate)\n",
        "  target_images.append(target_image)\n",
        "\n",
        "  # plot content, style, and target images\n",
        "  SF.plot_images([content_image, style_image, target_image])\n",
        "\n",
        "  # save target image\n",
        "  SF.save_image(target_image, 'target_image' + str(style_weight) + '.jpg')\n",
        "\n",
        "  # create video\n",
        "  SF.create_video(images, 'transformation_with_' + str(style_weight))\n",
        "\n",
        "plot_images_bunch(np.concatenate(([content_image, style_image], target_images)), names, nrows=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Style_Transfer Experiment 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}